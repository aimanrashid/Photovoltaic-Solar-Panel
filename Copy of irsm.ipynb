# **Setup**

## Mount Google drive

from google.colab import drive
drive.mount('/content/drive')

## Libraries

### Import libraries

import tensorflow as tf
# import tensorflow_addons as tfa

import os
import pickle
import tempfile

import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

import sklearn
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

from imblearn.over_sampling import SMOTENC

from collections import Counter

from tqdm.auto import tqdm, trange

tf.experimental.numpy.experimental_enable_numpy_behavior()

mpl.rcParams['figure.figsize'] = (12, 10)
colors = plt.rcParams['axes.prop_cycle'].by_key()['color']

tf.__version__

tf.config.list_physical_devices('GPU')[0]

tf.config.experimental.reset_memory_stats('GPU:0')

tf.config.experimental.get_memory_growth(tf.config.list_physical_devices('GPU')[0])

tf.config.experimental.get_memory_info('GPU:0')

physical_devices = tf.config.list_physical_devices('GPU')
try:
  tf.config.experimental.set_memory_growth(physical_devices[0], True)
  assert tf.config.experimental.get_memory_growth(physical_devices[0])
except:
  # Invalid device or cannot modify virtual devices once initialized.
  pass

## GPU initialization

tf.config.list_physical_devices()

!nvidia-smi

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

## TPU initialization

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
# This is the TPU initialization code that has to be at the beginning.
tf.tpu.experimental.initialize_tpu_system(resolver)
tf.config.list_logical_devices('TPU')

strategy = tf.distribute.TPUStrategy(resolver)

# **Load data**

## Data preprocessing

### Data reading

DATA_PATH = 'E:/Aiman Rashid/JupyterLab/InfraredSolarModules/images'
JSON_PATH = 'E:/Aiman Rashid/JupyterLab/InfraredSolarModules/module_metadata.json'

from re import findall
from os.path import join, basename
from glob import glob

def get_image_name(file_path: str):
    file_name = basename(file_path)
    return findall('[0-9]+', file_name)[-1]

images_paths = glob(join(DATA_PATH, '*.jpg'))
images_paths = sorted(images_paths, key=get_image_name)

images_path = [DATA_PATH + '/' + str(i) + '.jpg' for i in range(20000)]

### Get metadata

def get_metadata(metadata_path: str):
    with open(metadata_path, 'r', encoding='utf-8') as metadata_file:
        metadata = json.load(metadata_file)
    return pd.DataFrame(metadata)

metadata_df = get_metadata(metadata_path=JSON_PATH).transpose()
metadata_df['anomaly_class'] = metadata_df['anomaly_class'].astype('category')
metadata_df

file_name = [int(metadata_df['image_filepath'][i].split('/')[-1].split('.')[0]) for i in range(20000)]
metadata_df['filename'] = file_name
metadata_df = metadata_df.sort_values(by='filename')

classes = ['No-Anomaly', 'Cell', 'Cell-Multi', 'Cracking', 'Diode', 'Diode-Multi',
                  'Hot-Spot', 'Hot-Spot-Multi', 'Offline-Module',
                  'Shadowing', 'Soiling', 'Vegetation']

idx = []
for cate in tqdm(metadata_df['anomaly_class']):
  for i, cati in enumerate(classes):
    if cate == cati:
      idx.append(i)
    else: continue

metadata_df['category'] = idx
metadata_df['category'] = metadata_df['category'].astype('u1')

metadata_df

Counter(metadata_df['category']), Counter(metadata_df['anomaly_class'])

metadata_df.dtypes

### Read images

import cv2
from time import time


# def load_image(image_path):
#     return cv2.imread(image_path)

start = time()
images = [cv2.imread(image_path) for image_path in tqdm(images_path)]
print("Total time taken using list comprehension {} seconds".format (time() - start))

len(images)

images_3d = np.array(images)

images_3d.shape

images_2d = images_3d[:, :, :, 0]

images_2d.shape

labels = np.array(idx)

### Save images and labels

np.save('images_2d.npy', images_2d, allow_pickle=True, fix_imports=True)
np.save('images_3d.npy', images_3d, allow_pickle=True, fix_imports=True)
np.save('labels.npy', labels, allow_pickle=True, fix_imports=True)

## Load dataset

### Read dataset

DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/InfraredSolarModules'

X = np.load(os.path.join(DATA_PATH, 'images_2d.npy'),
            mmap_mode=None, allow_pickle=True, fix_imports=True, encoding='ASCII')
y = np.load(os.path.join(DATA_PATH, 'labels.npy'),
            mmap_mode=None, allow_pickle=True, fix_imports=True, encoding='ASCII')

with open(os.path.join(DATA_PATH, 'classes.pkl'), 'rb') as fp:
  classes = pickle.load(fp)

X, y = shuffle(X, y, random_state=42)

classes

fig, ax = plt.subplots(5, 4, figsize=(6, 12))
choices = []
for i in range(5):
    for j in range(4):
        choice = np.random.randint(0, 20000)
        ax[i, j].imshow(X[choice])
        ax[i, j].set_xticks([]), ax[i, j].set_yticks([])
        ax[i, j].set_title(f'{classes[y[choice]]}', fontsize=10, y=0.97)
        choices.append(choice)
plt.show()

### Resize dataset

X_resized = tf.cast(tf.image.resize(
          X.reshape(-1, 40, 24, 1),
          (X.shape[1]*6, X.shape[2]*6),
          method='nearest',
          preserve_aspect_ratio=False,
          antialias=False,
          name=None
          ), dtype='uint8')._numpy()

X_resized = X_resized.reshape(X_resized.shape[:3])
X_resized.shape

### Split and transform data

X_train, X_temp, y_train, y_temp = train_test_split(
    X, y,
    test_size=None,
    train_size=0.6,
    random_state=42,
    shuffle=True,
    stratify=y)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp,
    test_size=None,
    train_size=0.5,
    random_state=42,
    shuffle=True,
    stratify=y_temp)

scaler = MinMaxScaler()
X_train_n = scaler.fit_transform(X_train.reshape(-1, 1)).reshape(X_train.shape)
X_val_n = scaler.transform(X_val.reshape(-1, 1)).reshape(X_val.shape)
X_test_n = scaler.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)

def stats(X, name):
  print(f"{name}: min={X.min()}, mean={X.mean()}, std={X.std()}, max={X.max()}")

stats(X_train_n, "X_train_n")
stats(X_val_n, "X_val_n")
stats(X_test_n, "X_test_n")

#### Plot datasets

fig, ax = plt.subplots(5, 4, figsize=(6, 12))
choices = []
for i in range(5):
    for j in range(4):
        choice = np.random.randint(0, 12000)
        ax[i, j].imshow(X_train_n[choice])
        ax[i, j].set_xticks([]), ax[i, j].set_yticks([])
        ax[i, j].set_title(f'{classes[y_train[choice]]}', fontsize=10, y=0.97)
        choices.append(choice)
plt.show()

### Padding dataset

from numba import jit

X_n = [X_train_n, X_val_n, X_test_n]

@jit(nopython=False)
def padding(X):
  X_pad = [np.pad(X[i], pad_width=5, mode='constant',
                  constant_values=0) for i in range(len(X))]
  return np.array(X_pad)[:, :, :, np.newaxis]

X_pad = [padding(X_n[i]) for i in range(len(X_n))]
X_train_pad, X_val_pad, X_test_pad = X_pad

def resizing(x, min_t=0, max_t=1, name=None):

  if len(x.shape) < 4:
    x = x[..., tf.newaxis]

  resized = tf.image.resize(
      x, (x.shape[1]*6, x.shape[2]*6),
      method='lanczos5',
      preserve_aspect_ratio=False,
      antialias=False,
      name=name
      )
  if name == 'train':
    min_t, max_t = np.min(resized), np.max(resized)

  normalized = (resized - min_t) / (max_t - min_t)

  pad = 10
  padded = tf.image.pad_to_bounding_box(
    normalized,
    offset_height=pad,
    offset_width=pad,
    target_height=normalized.shape[1]+2*pad,
    target_width=normalized.shape[2]+2*pad
    )._numpy()

  # rgb = tf.image.grayscale_to_rgb(
  #     padded, name=name
  #     )
  if name == 'train':
    return padded, min_t, max_t
  else:
    return padded

X_train_pad, min_t, max_t = resizing(X_train, min_t=0, max_t=1, name='train')
X_val_pad = resizing(X_val, min_t=min_t, max_t=max_t, name='val')
X_test_pad = resizing(X_test, min_t=min_t, max_t=max_t, name='test')

X_train_pad.shape

choices = [np.random.randint(0, 12000) for _ in range(20)]

fig, ax = plt.subplots(5, 4, figsize=(6, 12))
for i in range(5):
    for j in range(4):
        # choice = np.random.randint(0, 12000)
        ax[i, j].imshow(X_train_pad[choices[4*i+j]])
        ax[i, j].set_xticks([]), ax[i, j].set_yticks([])
        ax[i, j].set_title(f'{classes[y_train[choices[4*i+j]]]}', fontsize=10, y=0.97)
plt.show()

### One-hot-encoded labels

from keras.utils import to_categorical

y_train_en = to_categorical(y_train)
y_val_en = to_categorical(y_val)
y_test_en = to_categorical(y_test)

y_train_en.shape

### TF datasests

train = tf.data.Dataset.from_tensor_slices((X_train_pad, y_train_en)).batch(1024)
val = tf.data.Dataset.from_tensor_slices((X_val_pad, y_val_en)).batch(1024)
test = tf.data.Dataset.from_tensor_slices((X_test_pad, y_test_en)).batch(1024)

train

# **DCNN**

## Build model

from tensorflow import keras
from tensorflow.keras import Model, Sequential
from tensorflow.keras.layers import (
    Activation, Average, BatchNormalization, Conv2D,
    Dense, Dropout, Input, Flatten, MaxPooling2D,
    GlobalMaxPooling2D)

METRICS = [
  keras.metrics.CategoricalCrossentropy(name='cat_ce'),  # same as model's loss
  keras.metrics.MeanSquaredError(name='Brier_score'),
  keras.metrics.TruePositives(name='tp'),
  keras.metrics.FalsePositives(name='fp'),
  keras.metrics.TrueNegatives(name='tn'),
  keras.metrics.FalseNegatives(name='fn'),
  keras.metrics.CategoricalAccuracy(name='accuracy'),
  keras.metrics.Precision(name='precision'),
  keras.metrics.Recall(name='recall'),
  keras.metrics.PrecisionAtRecall(recall=0.99, name='pr_rc'),
  keras.metrics.AUC(name='auc', curve='ROC', multi_label=True, num_labels=len(classes)),
  keras.metrics.AUC(name='prc', curve='PR', multi_label=True, num_labels=len(classes)), # precision-recall curve
  keras.metrics.SensitivityAtSpecificity(specificity=0.99, name='sens_spec'),
  keras.metrics.SpecificityAtSensitivity(sensitivity=0.99, name='spec_sens'),
  keras.metrics.MeanIoU(num_classes=len(classes), name='iou'),
  keras.metrics.CategoricalHinge(name='categorical_hinge'),
  keras.metrics.F1Score(average='weighted', name='f1_score'),
  # keras.metrics.FBetaScore(average='weighted', beta=1.0, threshold=None, name='fbeta_score'),
  keras.metrics.Poisson(name='poisson')
]

def make_model(activation):

    # Create a CNN model
    model = Sequential([
        # keras.Input(shape=X_train_2d.shape[1:]),

        Conv2D(filters=32, kernel_size=(3, 3),
               padding='valid',
               activation=activation,
               input_shape=X_train_pad.shape[1:],
               # kernel_initializer='glorot_uniform',
               # bias_initializer='glorot_uniform',
               ),
        BatchNormalization(),
        # Activation(activation=activation),
        MaxPooling2D(pool_size=(2, 2), padding='same'),

        Conv2D(filters=64, kernel_size=(3, 3),
               padding='same',
               activation=activation
               ),
        BatchNormalization(),
        # Activation(activation=activation),
        MaxPooling2D(pool_size=(2, 2), padding='same'),

        Conv2D(filters=128, kernel_size=(3, 3),
               padding='same',
               activation=activation
               ),
        BatchNormalization(),
        # Activation(activation=activation),
        MaxPooling2D(pool_size=(2, 2), padding='same'),

        Conv2D(filters=256, kernel_size=(3, 3),
               padding='same',
               activation=activation),
        MaxPooling2D(pool_size=(2, 2)),


        Conv2D(filters=512, kernel_size=(3, 3),
               padding='same',
               activation=activation),
        MaxPooling2D(pool_size=(2, 2), padding='same'),

        Conv2D(filters=1024, kernel_size=(3, 3),
               padding='same',
               activation=activation),
        MaxPooling2D(pool_size=(2, 2), padding='same'),

        # Flatten(),

        GlobalMaxPooling2D(),
        # Dense(256, activation=activation),
        Dropout(rate=0.2),

        Dense(units=2**10, activation=activation),
        BatchNormalization(),
        # Activation(activation=activation),
        Dropout(rate=0.3),

        Dense(units=2**8, activation=activation),
        BatchNormalization(),
        # Activation(activation=activation),
        Dropout(rate=0.4),

        Dense(units=2**6, activation=activation),
        BatchNormalization(),
        # Activation(activation=activation),
        Dropout(rate=0.5),

        Dense(len(classes), activation='softmax')  # 12 classes in your case
        ])

    METRICS = [
        keras.metrics.CategoricalAccuracy(name='accuracy'),
        keras.metrics.Precision(name='precision'),
        keras.metrics.Recall(name='recall'),
        keras.metrics.CategoricalCrossentropy(name='cat_ce'),  # same as model's loss
        keras.metrics.MeanSquaredError(name='Brier_score'),
        keras.metrics.TruePositives(name='tp'),
        keras.metrics.FalsePositives(name='fp'),
        keras.metrics.TrueNegatives(name='tn'),
        keras.metrics.FalseNegatives(name='fn'),
        keras.metrics.PrecisionAtRecall(recall=0.99, name='pr_rc'),
        keras.metrics.AUC(name='auc', curve='ROC', multi_label=True, num_labels=len(classes)),
        keras.metrics.AUC(name='prc', curve='PR', multi_label=True, num_labels=len(classes)), # precision-recall curve
        keras.metrics.SensitivityAtSpecificity(specificity=0.99, name='sens_spec'),
        keras.metrics.SpecificityAtSensitivity(sensitivity=0.99, name='spec_sens'),
        keras.metrics.MeanIoU(num_classes=len(classes), name='iou'),
        keras.metrics.CategoricalHinge(name='categorical_hinge'),
        keras.metrics.F1Score(average='weighted', name='f1_score'),
        keras.metrics.FBetaScore(average='weighted', beta=1.0, threshold=None, name='fbeta_score'),
        keras.metrics.Poisson(name='poisson')
      ]

    model.compile(
    optimizer=tf.keras.optimizers.Lion(learning_rate=1e-4,
                                       # weight_decay=5e-4,
                                       jit_compile=True,
                                       name='Lion'),
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=METRICS,
    # loss_weights=dict(enumerate(class_weights)),
    # weighted_metrics=dict(enumerate(class_weights)),
    )
    return model

def make_model(activation):

  # Variable-length int sequences.
  query_input = tf.keras.Input(shape=(None,), dtype='int32')
  value_input = tf.keras.Input(shape=(None,), dtype='int32')

  # Embedding lookup.
  token_embedding = tf.keras.layers.Embedding(input_dim=1024, output_dim=64)
  # Query embeddings of shape [batch_size, Tq, dimension].
  query_embeddings = token_embedding(query_input)
  # Value embeddings of shape [batch_size, Tv, dimension].
  value_embeddings = token_embedding(value_input)

  inputs = Input(shape=input_shape=X_train_pad.shape[1:])
  conv2d_0 = Conv2D(filters=32, kernel_size=(7, 7),
               padding='valid', activation=activation,
               # kernel_initializer='glorot_uniform',
               # bias_initializer='glorot_uniform',
               )(inputs)
  bn_0 = BatchNormalization()(conv2d_0)
  mp2d_0 = MaxPooling2D(pool_size=(2, 2))(bn_1)

  conv2d_1 = Conv2D(filters=64, kernel_size=(5, 5),
               padding='same', activation=activation,
               # kernel_initializer='glorot_uniform',
               # bias_initializer='glorot_uniform',
               )(mp2d_0)
  bn_1 = BatchNormalization()(conv2d_1)
  mp2d_1 = MaxPooling2D(pool_size=(2, 2))(bn_1)

  conv2d_2 = Conv2D(filters=128, kernel_size=(3, 3),
               padding='same', activation=activation,
               # kernel_initializer='glorot_uniform',
               # bias_initializer='glorot_uniform',
               )(mp2d_1)
  bn_2 = BatchNormalization()(conv2d_1)
  mp2d_20 = MaxPooling2D(pool_size=(2, 2))(bn_1)



    # Create a CNN model
    model = Sequential([
        # keras.Input(shape=X_train_2d.shape[1:]),

        Conv2D(filters=32, kernel_size=(7, 7),
               padding='valid', activation=activation,
               input_shape=X_train_pad.shape[1:],
               # kernel_initializer='glorot_uniform',
               # bias_initializer='glorot_uniform',
               ),
        BatchNormalization(),
        # Activation(activation=activation),
        MaxPooling2D(pool_size=(2, 2)),

        Conv2D(filters=64, kernel_size=(5, 5),
               padding='same', activation=activation),
        BatchNormalization(),
        # Activation(activation=activation),
        MaxPooling2D(pool_size=(2, 2)),

        Conv2D(filters=128, kernel_size=(3, 3),
               padding='same', activation=activation),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 2)),

        # Conv2D(filters=256, kernel_size=(1, 1),
                            # activation=activation),
        # MaxPooling2D(pool_size=(2, 2)),

        # Query encoding of shape [batch_size, Tq, filters].
        query_seq_encoding = cnn_layer(query_embeddings)
        # Value encoding of shape [batch_size, Tv, filters].
        value_seq_encoding = cnn_layer(value_embeddings)


        # Flatten(),

        GlobalMaxPooling2D(),
        # Dense(256, activation=activation),
        Dropout(0.5),

        Dense(units=1024, activation=activation),
        BatchNormalization(),
        Dropout(rate=0.5),

        Dense(units=512, activation=activation),
        BatchNormalization(),
        Dropout(rate=0.5),

        Dense(len(classes), activation='softmax')  # 12 classes in your case
        ])

    METRICS = [
        keras.metrics.CategoricalAccuracy(name='accuracy'),
        keras.metrics.Precision(name='precision'),
        keras.metrics.Recall(name='recall'),
        keras.metrics.CategoricalCrossentropy(name='cat_ce'),  # same as model's loss
        keras.metrics.MeanSquaredError(name='Brier_score'),
        keras.metrics.TruePositives(name='tp'),
        keras.metrics.FalsePositives(name='fp'),
        keras.metrics.TrueNegatives(name='tn'),
        keras.metrics.FalseNegatives(name='fn'),
        keras.metrics.PrecisionAtRecall(recall=0.99, name='pr_rc'),
        keras.metrics.AUC(name='auc', curve='ROC', multi_label=True, num_labels=len(classes)),
        keras.metrics.AUC(name='prc', curve='PR', multi_label=True, num_labels=len(classes)), # precision-recall curve
        keras.metrics.SensitivityAtSpecificity(specificity=0.99, name='sens_spec'),
        keras.metrics.SpecificityAtSensitivity(sensitivity=0.99, name='spec_sens'),
        keras.metrics.MeanIoU(num_classes=len(classes), name='iou'),
        keras.metrics.CategoricalHinge(name='categorical_hinge'),
        keras.metrics.F1Score(average='weighted', name='f1_score'),
        keras.metrics.FBetaScore(average='weighted', beta=1.0, threshold=None, name='fbeta_score'),
        keras.metrics.Poisson(name='poisson')
      ]

    model.compile(
    optimizer=tf.keras.optimizers.Lion(learning_rate=1e-4,
                                       # weight_decay=5e-4,
                                       jit_compile=True,
                                       name='Lion'),
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=METRICS,
    # loss_weights=dict(enumerate(class_weights)),
    # weighted_metrics=dict(enumerate(class_weights)),
    )
    return model

keras.backend.clear_session()

# early_stopping, checkpoint, tensorbard_callback = callbacks_(999)

with tf.device('/device:GPU:0'):
# with strategy.scope():
  model = make_model(tf.keras.layers.LeakyReLU(alpha=0.6))

  model.summary()

2**16, 2**14, 2**12

import math
from tensorflow.keras.callbacks import LearningRateScheduler

def lr_schedule(epoch):
    initial_lr = 1e-4
    drop = 0.75
    epochs_drop = 25
    lr = initial_lr * math.pow(drop, math.floor((1 + epoch) / epochs_drop))
    return lr

lr_scheduler = LearningRateScheduler(lr_schedule)

def decayed_learning_rate(step):
  initial_learning_rate = 1e-4
  decay_rate = 0.75
  decay_steps = 50
  return initial_learning_rate * decay_rate ** (step / decay_steps)

plt.figure(figsize=(5, 5))
plt.plot([decayed_learning_rate(i) for i in range(500)])

plt.figure(figsize=(5, 5))
plt.plot(history.history['val_loss'])

lr_schedule(100)

plt.plot(lr_schedule(np.linspace(500).reshape(-1, 1)))

os.path.join(DATA_PATH, 'multiclassification_CNN/checkpoints/cp_' + str(699))

EPOCHS = 1000
BATCH_SIZE = 512

def callbacks_(i):
  early_stopping = tf.keras.callbacks.EarlyStopping(
      monitor='val_accuracy',
      min_delta=0.01,
      verbose=1,
      patience=200,
      mode='max',
      baseline=None,
      restore_best_weights=True,
      start_from_epoch=200
      )

  checkpoint_filepath = os.path.join(DATA_PATH, 'multiclassification_CNN/checkpoints_kf/cp_' + str(i))
  checkpoint = tf.keras.callbacks.ModelCheckpoint(
      filepath=checkpoint_filepath,
      monitor='val_accuracy',
      verbose=1,
      save_best_only=True,
      save_weights_only=True,
      mode='max',
      save_freq='epoch',
      options=None,
      initial_value_threshold=None,
      )

  # log_dirpath = os.path.join(DATA_PATH, 'multiclassification_CNN/logs/log_' + str(i))
  # tensorboard_callback = tf.keras.callbacks.TensorBoard(
  #     log_dir=log_dirpath,
  #     histogram_freq=1,
  #     write_graph=True,
  #     write_images=True,
  #     write_steps_per_second=True,
  #     update_freq='epoch',
  #     profile_batch=5,
  #     embeddings_freq=1,
  #     embeddings_metadata=None
  #     )

  # lr_scheduler = LearningRateScheduler(lr_schedule)

  return early_stopping, checkpoint

### Model visualization

tf.keras.backend.clear_session()
model = make_model(tf.keras.layers.LeakyReLU(alpha=0.6))
model.summary()

tf.keras.utils.plot_model(
    model,
    to_file=os.path.join(DATA_PATH, 'multiclassification_CNN/model.png'),
    show_shapes=True,
    show_dtype=True,
    show_layer_names=True,
    rankdir="TB",
    expand_nested=True,
    dpi=96,
    layer_range=None,
    show_layer_activations=True,
    show_trainable=True)

## Train model

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

keras.backend.clear_session()

# early_stopping, checkpoint, tensorbard_callback = callbacks_(999)

with tf.device('/device:GPU:0'):
# with strategy.scope():
  model = make_model(tf.keras.layers.LeakyReLU(alpha=0.6))
  # model = make_model('relu')

  # Compute class weights to handle class imbalance
  class_weights = compute_class_weight('balanced',
                                      classes=np.unique(y_train),
                                      y=y_train)
  METRICS = [
    keras.metrics.CategoricalAccuracy(name='accuracy'),
    keras.metrics.Precision(name='precision'),
    keras.metrics.Recall(name='recall'),
    keras.metrics.CategoricalCrossentropy(name='cat_ce'),  # same as model's loss
    keras.metrics.MeanSquaredError(name='Brier_score'),
    keras.metrics.TruePositives(name='tp'),
    keras.metrics.FalsePositives(name='fp'),
    keras.metrics.TrueNegatives(name='tn'),
    keras.metrics.FalseNegatives(name='fn'),
    keras.metrics.PrecisionAtRecall(recall=0.99, name='pr_rc'),
    keras.metrics.AUC(name='auc', curve='ROC', multi_label=True, num_labels=len(classes)),
    keras.metrics.AUC(name='prc', curve='PR', multi_label=True, num_labels=len(classes)), # precision-recall curve
    keras.metrics.SensitivityAtSpecificity(specificity=0.99, name='sens_spec'),
    keras.metrics.SpecificityAtSensitivity(sensitivity=0.99, name='spec_sens'),
    keras.metrics.MeanIoU(num_classes=len(classes), name='iou'),
    keras.metrics.CategoricalHinge(name='categorical_hinge'),
    keras.metrics.F1Score(average='weighted', name='f1_score'),
    keras.metrics.FBetaScore(average='weighted', beta=1.0, threshold=None, name='fbeta_score'),
    keras.metrics.Poisson(name='poisson')
  ]

model.compile(
    optimizer=tf.keras.optimizers.Lion(
        learning_rate=1e-4,
        # tf.keras.optimizers.schedules.ExponentialDecay(
        #     initial_learning_rate=1e-4,
        #     decay_steps=25,
        #     decay_rate=0.75,
        #     staircase=False,
        #     name=None),
        # weight_decay=5e-4,
        jit_compile=True),
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=METRICS,
    # loss_weights=dict(enumerate(class_weights)),
    # weighted_metrics=dict(enumerate(class_weights)),
    )

history = model.fit(
    train,
    # x=X_train_pad,
    # y=y_train_en,
    # batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    verbose="auto",
    callbacks=[callbacks_(401)],
    validation_split=0.0,
    # validation_data=(X_val_pad, y_val_en),
    validation_data=val,
    shuffle=True,
    class_weight=dict(enumerate(class_weights)),
    sample_weight=None,
    initial_epoch=0,
    steps_per_epoch=None,
    validation_steps=None,
    # validation_batch_size=BATCH_SIZE,
    validation_freq=1,
    max_queue_size=25,
    workers=16,
    use_multiprocessing=True,
    )

with open(os.path.join(DATA_PATH, 'multiclassification_CNN/histories/history_401'), 'wb') as fp:
  pickle.dump(history.history, fp)

results = model.evaluate(test)
preds = model.predict(X_test_pad)

with open(os.path.join(DATA_PATH, 'multiclassification_CNN/histories/history_999'), 'rb') as fp:
  pickle.load(fp)

results = model.evaluate(X_test_pad, y_test_en)
preds = model.predict(X_test_pad)

results_train = model.evaluate(X_train_pad, y_train_en)
preds_train = model.predict(X_train_pad)

## Apply PCC

model.summary()

X_train_pad.shape

X_pad = np.concatenate((X_train_pad, X_val_pad, X_test_pad))
y_en = np.concatenate((y_train_en, y_val_en, y_test_en))

X_pad.shape, y_en.shape

#Load the best model for feature extraction
# model = tf.keras.models.load_model('./best_model.h5')
intermediate_layer_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer('dense_1').output)
intermediate_layer_model.summary()

#Extract the features
feature_engg_data = intermediate_layer_model.predict(X_pad)
feature_engg_data = pd.DataFrame(feature_engg_data)

# PCA apply
from sklearn.preprocessing import StandardScaler
x = feature_engg_data.loc[:, feature_engg_data.columns].values
x = StandardScaler().fit_transform(x)

x_train, x_test, Y_train, Y_test = train_test_split(x, y_en, test_size=0.2, stratify=y_en, random_state=42)

from sklearn.feature_selection import mutual_info_classif

mi_class_corr = mutual_info_classif(normalizeddf_train)

type(normalizeddf_train)

# Finding correlation
from sklearn.preprocessing import Normalizer
normalizeddf_train = Normalizer().fit_transform(x_train)
normalizeddf_test = Normalizer().fit_transform(x_test)
print (normalizeddf_train)
print(pd.DataFrame(normalizeddf_train).corr(method='pearson'))

def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = pd.DataFrame(dataset).corr(method='pearson')
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr
corr_features = correlation(normalizeddf_train, 0.85)  #Set the correlation value
print(len(set(corr_features)))

# Drop the correlated features
X_train = pd.DataFrame(normalizeddf_train).drop(corr_features,axis=1)
X_test = pd.DataFrame(normalizeddf_test).drop(corr_features,axis=1)

# Custom Extreme Learning Machine
input_size = X_train.shape[1]
hidden_size = 1500

input_weights = np.random.normal(size=[input_size, hidden_size])
biases = np.random.normal(size=[hidden_size])

import scipy

def relu(x):
   return np.maximum(x, 0, x)

def hidden_nodes(X):
    G = np.dot(X, input_weights)
    G = G + biases
    H = relu(G)
    return H

output_weights = np.dot(scipy.linalg.pinv(hidden_nodes(X_train)), Y_train)

def predict(X):
    out = hidden_nodes(X)
    out = np.dot(out, output_weights)
    return out

prediction = predict(X_test)

from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, auc
from sklearn import metrics

# Calculate the classification results
rounded_labels=np.argmax(prediction, axis=1)
real = np.argmax(Y_test, axis=1)
print(confusion_matrix(real, rounded_labels))
print(classification_report(real, rounded_labels))
print(metrics.accuracy_score(real, rounded_labels))
rounded_labels=np.argmax(prediction, axis=1)
real = np.argmax(Y_train, axis=1)

preds

prediction

cm(preds=prediction, true=np.argmax(Y_test, axis=1), title='Test')

#ROC
import matplotlib.pyplot as plt
from itertools import cycle

prediction = predict(X_test)
lw = 1
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(8):
    fpr[i], tpr[i], _ = roc_curve(Y_test[:, i], prediction[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])


fpr["micro"], tpr["micro"], _ = roc_curve(Y_test.ravel(), prediction.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])


colors = cycle([ 'red', 'blue', 'green', 'yellow', 'cyan', 'red', 'blue', 'green',])
for i, color in zip(range(8), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=lw, label='ROC curve of class {0} (area = {1:0.4f})'
                                                        ''.format(i, roc_auc[i]))


plt.plot([0, 1], [0, 1], 'k--', lw=lw)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC for Classification of IR Thermal Images using CNN-PCC-ELM ')
plt.legend(loc="lower right")
plt.show()
plt.pause(1)
c = roc_auc_score(Y_test, prediction, multi_class='ovo')
print("AUC:", c)


## Startified K-fold cross validation

class_weights = compute_class_weight(
    'balanced',
    classes=np.unique(y_train),
    y=y_train)

def train_model(model, X_train, y_train, X_val, y_val):
  
  history = model.fit(
      x=X_train,
      y=y_train,
      batch_size=BATCH_SIZE,
      epochs=EPOCHS,
      verbose="auto",
      callbacks=[early_stopping, checkpoint],
      validation_split=0.0,
      validation_data=(X_val, y_val),
      shuffle=True,
      class_weight=dict(enumerate(class_weights)),
      sample_weight=None,
      initial_epoch=0,
      steps_per_epoch=None,
      validation_steps=None,
      validation_batch_size=BATCH_SIZE,
      validation_freq=1,
      max_queue_size=25,
      workers=16,
      use_multiprocessing=True,
      )
  return history

40*6+20

X_train_pad.shape

Xsplit = np.vstack((X_train_pad, X_val_pad))
ysplit = np.vstack((y_train.reshape(-1, 1), y_val.reshape(-1, 1))).ravel()
Xsplit.shape, ysplit.shape

{*zip(history.history.keys(), results)}

from sklearn.model_selection import StratifiedKFold

# Instantiate the cross validator
kfold_splits = 20
skf = StratifiedKFold(n_splits=kfold_splits, shuffle=True)

#
models, histories, results, preds = [], [], [], []

# Loop through the indices the split() method returns
for index, (train_indices, val_indices) in enumerate(skf.split(Xsplit, ysplit)):
    print("Training on fold " + str(index+1) + "/" + str(kfold_splits) + "...")

    # Generate batches from indices
    xtrain, xval = Xsplit[train_indices], Xsplit[val_indices]
    ytrain, yval = ysplit[train_indices], ysplit[val_indices]
    ytrain, yval = to_categorical(ytrain), to_categorical(yval)

    # Define callbacks
    early_stopping, checkpoint = callbacks_(index)

    # Clear model, and create it
    with tf.device('/device:GPU:0'):
    # with strategy.scope():
      # model = None
      models.append(make_model(tf.keras.layers.LeakyReLU(alpha=0.6)))

    # Debug message I guess
    # print "Training new iteration on " + str(xtrain.shape[0]) + " training samples, " + str(xval.shape[0]) + " validation samples, this may be a while..."

    histories.append(train_model(models[index], xtrain, ytrain, xval, yval))
    results.append(models[index].evaluate(X_test_pad, y_test_en))
    preds.append(models[index].predict(X_test_pad))

    with open(os.path.join(DATA_PATH, f'multiclassification_CNN/histories_kf/history_{index}'), 'wb') as fp:
      pickle.dump(history.history, fp)
print("\nThe Startified K-Fold Cross-Validation has finished.\n")

    # accuracy_history = histories[index].history['accuracy']
    # val_accuracy_history = histories[index].history['val_accuracy']
    # print(f"Last training accuracy: {accuracy_history[-1]}",
    #       f"Last validation accuracy: {val_accuracy_history[-1]}")

def ensembleModels(models, model_input):
    # collect outputs of models in a list
    yModels=[model(model_input) for model in models] 
    # averaging outputs
    yAvg=keras.layers.average(yModels) 
    # build model from same input and avg output
    with tf.device('/device:GPU:0'):
      modelEns = Model(inputs=model_input, outputs=yAvg, name='ensemble')  
   
    return modelEns

model_input = Input(shape=models[0].input_shape[1:]) # c*h*w
modelEns = ensembleModels(models, model_input)
modelEns.summary()

resultEns = modelEns.results(X_test_pad, y_test_en)
predsEns = modelEns.predict(X_test_pad)

results = model.evaluate(X_test_pad, y_test_en)
preds = model.predict(X_test_pad)

## Check trained model on K-Fold CV

early_stopping, checkpoint, _ = callbacks_(5)

model = make_model(tf.keras.layers.LeakyReLU(alpha=0.6))

models, results, preds = [], [], []
for i in range(10):
  checkpoint_filepath = os.path.join(DATA_PATH, 'multiclassification_CNN/checkpoints/cp_' + str(i))
  models.append(make_model(tf.keras.layers.LeakyReLU(alpha=0.6)))
  models[i].load_weights(checkpoint_filepath)
  results.append(model.evaluate(X_test_pad, y_test_en))
  preds.append(model.predict(X_test_pad))

len(preds)

results = model.evaluate(X_test_pad, y_test_en)
preds = model.predict(X_test_pad)

## Performance

### Loss and Accuracy

def plot_metrics(history):
  metrics = ['loss', 'accuracy', 'prc', 'auc', 'precision', 'recall', 'iou',
             'categorical_hinge', 'f1_score', 'fbeta_score']
  for n, metric in enumerate(metrics):
    # name = metric.replace("_"," ").capitalize()
    name = metric.capitalize()
    plt.subplot(5,2,n+1)
    plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')
    plt.plot(history.epoch, history.history['val_'+metric],
             color=colors[1], linestyle="--", label='Val')
    plt.xlabel('Epoch')
    plt.ylabel(name)
    plt.legend()

### Confusion matrix

def cm(preds, true, title):
  fig, ax = plt.subplots(figsize=(12, 12))
  ConfusionMatrixDisplay(
      confusion_matrix(
          true,
          np.argmax(preds, axis=1),
          labels=np.arange(len(classes))),
      display_labels=classes.values()).plot(
          cmap='Blues', ax=ax, xticks_rotation='vertical',
          colorbar=False, text_kw=dict(fontfamily='monospace'))

  pct = confusion_matrix(
    true,
    np.argmax(preds, axis=1),
    labels=np.arange(len(classes))).diagonal() / np.unique(
        true, return_counts=True)[1] * 100

  for i in range(len(classes)):
    plt.text(i, i, f'{pct[i]:.2f}%',
              fontdict=dict(ha='center',
                            va='bottom',
                            position=(i, i+0.35),
                            color='Red',
                            fontsize=10,
                            fontfamily='monospace'))
    plt.title(f'Confusion Matrix {title} ({len(classes)} classes)')

  plt.show()

### Metrics

plot_metrics(history=history)

cm(preds=preds, true=y_test, title='Test')

print(classification_report(np.argmax(preds, axis=1), y_test))

## Tensorboard

model = make_model

# Load the TensorBoard notebook extension
%load_ext tensorboard

import datetime

os.path.isdir(os.path.join(DATA_PATH, 'multiclassification_CNN/logs/log_799'))

os.path.join(DATA_PATH, 'multiclassification_CNN/logs/log_799')

!ls "/content/drive/MyDrive/Colab Notebooks/InfraredSolarModules/multiclassification_CNN/logs/log_799"

!pip install -U tensorboard-plugin-profile --quiet

%tensorboard --logdir='/content/drive/MyDrive/Colab Notebooks/InfraredSolarModules/multiclassification_CNN/logs/log_799'

# Test resizing

from matplotlib import pyplot as plt

img = plt.imread(r'/content/drive/MyDrive/Colab Notebooks/InfraredSolarModules/tv-signal-test-screen-retro-television-broadcast-vector-45811635.jpg')
fig, ax = plt.subplots(1, 4, figsize=(16, 20))
ax[0].imshow(img)
cmaps = ['Reds', 'Greens', 'Blues']
for i in range(1, 4):
  ax[i].imshow(img[:, :, i-1], cmap=cmaps[i-1])

import tensorflow as tf

methods = ['bilinear', 'lanczos3', 'lanczos5', 'bicubic', 'gaussian', 'nearest', 'area', 'mitchellcubic']
resized0s, resized1s = [], []

resized0s = [tf.cast(tf.image.resize(
    img,
    (40, 24),
    method=method,
    preserve_aspect_ratio=True,
    antialias=False,
    name=None
), dtype='uint8') for method in methods]

resized1s =[tf.cast(tf.image.resize(
    resized0,
    (857, 1000),
    method=method,
    preserve_aspect_ratio=False,
    antialias=False,
    name=None
), dtype='uint8') for method, resized0 in zip(methods, resized0s)]

resized2s = [tf.cast(tf.image.resize(
    img,
    (8570, 10000),
    method=method,
    preserve_aspect_ratio=True,
    antialias=False,
    name=None
), dtype='uint8') for method in methods]

resized3s =[tf.cast(tf.image.resize(
    resized2,
    (857, 1000),
    method=method,
    preserve_aspect_ratio=False,
    antialias=False,
    name=None
), dtype='uint8') for method, resized2 in zip(methods, resized2s)]

224/40, 224/24

fig, ax = plt.subplots(len(methods), 5, figsize=(16, 20), layout='constrained')
for i, (method, resized0, resized1, resized2, resized3) in enumerate(zip(methods, resized0s, resized1s, resized2s, resized3s)):
  ax[i, 0].imshow(img), ax[0, 0].set_title('Original'), ax[i, 0].set_ylabel(method)
  ax[i, 1].imshow(resized0), ax[0, 1].set_title(f'{resized0.shape[:2]}')
  ax[i, 2].imshow(resized1), ax[0, 2].set_title(f'{resized1.shape[:2]}')
  ax[i, 3].imshow(resized2), ax[0, 3].set_title(f'{resized2.shape[:2]}')
  ax[i, 4].imshow(resized3), ax[0, 4].set_title(f'{resized3.shape[:2]}')
plt.show()

img.dtype

tf.experimental.numpy.experimental_enable_numpy_behavior()

resized0.min(), resized0.max()

img.shape, resized.shape

resized = [tf.cast(tf.image.resize(
    img,
    (40, 24),
    method=method,
    preserve_aspect_ratio=True,
    antialias=False,
    name=None
), dtype='uint8') for method in methods]

fig, ax = plt.subplots(len(classes), len(methods)+1, figsize=(10, 20))
plt.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9, wspace=0.05, hspace=0.05)

for i in range(len(classes)):
  img = X[y==i][0][..., tf.newaxis]
  for j in range(len(methods)+1):
    ax[0, 0].set_title('Original', fontsize=10)
    if j == 0:
      ax[i, j].imshow(img)
      ax[i, j].set_ylabel(classes[i].capitalize())
    else:
      resized = tf.cast(tf.image.resize(
          img,
          (40*8, 24*8),
          method=methods[j-1],
          preserve_aspect_ratio=False,
          antialias=False,
          name=None
          ), dtype='uint8')
      ax[i, j].imshow(resized)
      if i == 0:
        ax[i, j].set_title(methods[j-1].capitalize(), fontsize=10)
    ax[i, j].set_xticks([]), ax[i, j].set_yticks([])

resized.shape

img.shape[0]*7, img.shape[1]*7

# **Outlier study**

## Within class

X[y==1].reshape(-1, 40*24).shape


import time

import matplotlib
import matplotlib.pyplot as plt
import numpy as np

from sklearn import svm
from sklearn.covariance import EllipticEnvelope
from sklearn.datasets import make_blobs, make_moons
from sklearn.ensemble import IsolationForest
from sklearn.kernel_approximation import Nystroem
from sklearn.linear_model import SGDOneClassSVM
from sklearn.neighbors import LocalOutlierFactor
from sklearn.pipeline import make_pipeline

matplotlib.rcParams["contour.negative_linestyle"] = "solid"

# Example settings
n_samples = 300
outliers_fraction = 0.15
n_outliers = int(outliers_fraction * n_samples)
n_inliers = n_samples - n_outliers

# define outlier/anomaly detection methods to be compared.
# the SGDOneClassSVM must be used in a pipeline with a kernel approximation
# to give similar results to the OneClassSVM
anomaly_algorithms = [
    (
        "Robust covariance",
        EllipticEnvelope(contamination=outliers_fraction, random_state=42),
    ),
    ("One-Class SVM", svm.OneClassSVM(nu=outliers_fraction, kernel="rbf", gamma=0.1)),
    (
        "One-Class SVM (SGD)",
        make_pipeline(
            Nystroem(gamma=0.1, random_state=42, n_components=150),
            SGDOneClassSVM(
                nu=outliers_fraction,
                shuffle=True,
                fit_intercept=True,
                random_state=42,
                tol=1e-6,
            ),
        ),
    ),
    (
        "Isolation Forest",
        IsolationForest(contamination=outliers_fraction, random_state=42),
    ),
    (
        "Local Outlier Factor",
        LocalOutlierFactor(n_neighbors=35, contamination=outliers_fraction),
    ),
]

# Define datasets
blobs_params = dict(random_state=0, n_samples=n_inliers, n_features=2)
datasets = [
    make_blobs(centers=[[0, 0], [0, 0]], cluster_std=0.5, **blobs_params)[0],
    make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[0.5, 0.5], **blobs_params)[0],
    make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[1.5, 0.3], **blobs_params)[0],
    4.0
    * (
        make_moons(n_samples=n_samples, noise=0.05, random_state=0)[0]
        - np.array([0.5, 0.25])
    ),
    14.0 * (np.random.RandomState(42).rand(n_samples, 2) - 0.5),
]

datasets = [X[y==1].reshape(-1, 40*24)]

# Compare given classifiers under given settings
xx, yy = np.meshgrid(np.linspace(-7, 7, 150), np.linspace(-7, 7, 150))

plt.figure(figsize=(len(anomaly_algorithms) * 2 + 4, 12.5))
plt.subplots_adjust(
    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01
)

plot_num = 1
rng = np.random.RandomState(42)

for i_dataset, X_ in enumerate(datasets):
    # Add outliers
    # X_ = np.concatenate([X_, rng.uniform(low=-6, high=6, size=(n_outliers, 2))], axis=0)

    for name, algorithm in anomaly_algorithms:
        t0 = time.time()
        algorithm.fit(X_)
        t1 = time.time()
        plt.subplot(len(datasets), len(anomaly_algorithms), plot_num)
        if i_dataset == 0:
            plt.title(name, size=18)

        # fit the data and tag outliers
        if name == "Local Outlier Factor":
            y_pred = algorithm.fit_predict(X_)
        else:
            y_pred = algorithm.fit(X_).predict(X_)

        # plot the levels lines and the points
        if name != "Local Outlier Factor":  # LOF does not implement predict
            Z = algorithm.predict(np.c_[xx.ravel(), yy.ravel()])
            Z = Z.reshape(xx.shape)
            plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors="black")

        colors = np.array(["#377eb8", "#ff7f00"])
        plt.scatter(X_[:, 0], X_[:, 1], s=10, color=colors[(y_pred + 1) // 2])

        plt.xlim(-7, 7)
        plt.ylim(-7, 7)
        plt.xticks(())
        plt.yticks(())
        plt.text(
            0.99,
            0.01,
            ("%.2fs" % (t1 - t0)).lstrip("0"),
            transform=plt.gca().transAxes,
            size=15,
            horizontalalignment="right",
        )
        plot_num += 1

plt.show()

 from sklearn.svm import OneClassSVM

clf = OneClassSVM(gamma='auto').fit(X[y==1].reshape(-1, 40*24))
preds = clf.predict(X[y==1].reshape(-1, 40*24))
scores = clf.score_samples(X[y==1].reshape(-1, 40*24))

preds.shape

[x for x in np.where(preds==1)[0]]

fig, ax = plt.subplots(1, len(np.where(preds==1)[0]))
for i, x in enumerate(np.where(preds==1)[0]):
  ax[i].imshow(X[y==1][x])


plt.plot(preds.reshape(-1, 1), 'o')

colors = np.array(["#377eb8", "#ff7f00"])
plt.scatter(X.[:, 0], X[:, 1], s=10, color=colors[(y_pred + 1) // 2])

cd /content/drive/MyDrive/Colab Notebooks

!git clone -l -s https://github.com/jenniferjang/dbscanpp.git dbscanpp_repo

cd dbscanpp_repo

ls

!pip install dbscanpp --quiet

from dbscanpp.DBSCANPP import DBSCANPP
import numpy as np

# Mixture of three multivariate Gaussians
cov = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]

X = np.concatenate([np.random.multivariate_normal([5, 5, 5], cov, 1000),
          np.random.multivariate_normal([0, 0, 0], cov, 1000),
          np.random.multivariate_normal([-5, -5, -5], cov, 1000)])
y = np.concatenate([np.full(1000, 0), np.full(1000, 1), np.full(1000, 2)])

# Declare a DBSCAN++ model with tuning hyperparameters
dbscanpp = DBSCANPP(p=0.1, eps_density=5.0, eps_clustering=5.0, minPts=10)
y_hat = dbscanpp.fit_predict(X, init="k-center")

# Score the clustering
from sklearn.metrics.cluster import adjusted_rand_score, adjusted_mutual_info_score
print("Adj. Rand Index Score: %f." % adjusted_rand_score(y_hat, y))
print("Adj. Mutual Info Score: %f." % adjusted_mutual_info_score(y_hat, y))

!pip install hdbscan --quiet

import hdbscan

clusterer = hdbscan.HDBSCAN(min_cluster_size=4, metric='haversine',
cluster_selection_epsilon=epsilon, cluster_selection_method = 'eom')
clusterer.fit(X[y==1])

from sklearn.manifold import TSNE

projection = TSNE().fit_transform(X[y==1].reshape(-1, 40*24))
plt.scatter(*X[y==1].reshape(-1, 40*24).T, s=50, linewidth=0, c='b', alpha=0.25)

X[y==1].reshape(-1, 40*24).T.shape

clusterer = hdbscan.HDBSCAN(min_cluster_size=15).fit(X[y==1].reshape(-1, 40*24))
color_palette = sns.color_palette('deep', 8)
cluster_colors = [color_palette[x] if x >= 0
                  else (0.5, 0.5, 0.5)
                  for x in clusterer.labels_]
cluster_member_colors = [sns.desaturate(x, p) for x, p in
                         zip(cluster_colors, clusterer.probabilities_)]
plt.scatter(*X[y==1].reshape(-1, 40*24).T, linewidth=0, alpha=0.25)

import hdbscan

clusterer = hdbscan.HDBSCAN(algorithm='best', alpha=1.0, approx_min_span_tree=True,
                            gen_min_span_tree=False, leaf_size=40,
                            # memory=None,
                            metric='euclidean', min_cluster_size=5, min_samples=None, p=None)

clusterer.fit(X[y==1].reshape(-1, 40*24))

!pip install pacmap --quiet

Counter(y)

classes

import pacmap
import numpy as np
import matplotlib.pyplot as plt

# loading preprocessed coil_20 dataset
# you can change it with any dataset that is in the ndarray format, with the shape (N, D)
# where N is the number of samples and D is the dimension of each sample
# X = np.load("./data/coil_20.npy", allow_pickle=True)
# X = X.reshape(X.shape[0], -1)
# y = np.load("./data/coil_20_labels.npy", allow_pickle=True)

# initializing the pacmap instance
# Setting n_neighbors to "None" leads to a default choice shown below in "parameter" section
embedding = pacmap.PaCMAP(n_components=2, n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0)

# fit the data (The index of transformed data corresponds to the index of the original data)
X_transformed = embedding.fit_transform(X[y==10].reshape(-1, 40*24), init="pca")

# visualize the embedding
fig, ax = plt.subplots(1, 1, figsize=(6, 6))
ax.scatter(X_transformed[:, 0], X_transformed[:, 1], cmap="Spectral", c=y[y==10], s=0.6)

# fit the data (The index of transformed data corresponds to the index of the original data)
X_transformed = embedding.fit_transform(X[y==11].reshape(-1, 40*24), init="pca")

# visualize the embedding
fig, ax = plt.subplots(1, 1, figsize=(6, 6))
ax.scatter(X_transformed[:, 0], X_transformed[:, 1], cmap="Spectral", c=y[y==11], s=0.6)

plt.imshow(X[y==1][10])
plt.title(f"{classes[1]}")
for i in range(X[y==1][10].shape[0]):
  for j in range(X[y==1][10].shape[1]):
    if ((i < 4) or (i > 35)):
      if ((j < 4) or (j > 19)):
        plt.text(j, i, s=f"{X[y==1][10, i, j]}", fontsize=6, ha='center', va='center')

data = X[y==1].reshape(-1, 40*24)
plt.scatter(*data.T, s=50, linewidth=0, c='b', alpha=0.25)

data.T.shape

import hdbscan

clusterer = hdbscan.HDBSCAN(algorithm='best', alpha=1.0, approx_min_span_tree=True,
                            gen_min_span_tree=False, leaf_size=40,
                            metric='euclidean', min_cluster_size=5,
                            min_samples=None, p=None)
fitted = clusterer.fit(data)

clusterer.labels_

Counter(clusterer.labels_)

clusterer.labels_.max()

clusterer.probabilities_

hdbscan.dist_metrics.METRIC_MAPPING

plt.scatter(*data.T)
